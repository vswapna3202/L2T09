{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}],"source":["import pandas as pd\n","import fuzzywuzzy\n","from fuzzywuzzy import process\n","import datetime\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"lqt_yzRy16Wj"},"source":["## Compulsory Task \n","\n","In this compulsory task you will clean the country column and parse the date column in the **store_income_data_task.csv** file."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vBP3WN2O16Wp"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Cullen/Frost Bankers, Inc.</td>\n","      <td>NaN</td>\n","      <td>Clothing</td>\n","      <td>$54438554.24</td>\n","      <td>4-2-2006</td>\n","      <td>United States/</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Nordson Corporation</td>\n","      <td>NaN</td>\n","      <td>Tools</td>\n","      <td>$41744177.01</td>\n","      <td>4-1-2006</td>\n","      <td>Britain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Stag Industrial, Inc.</td>\n","      <td>NaN</td>\n","      <td>Beauty</td>\n","      <td>$36152340.34</td>\n","      <td>12-9-2003</td>\n","      <td>United States</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>Britain/</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mercantile Bank Corporation</td>\n","      <td>NaN</td>\n","      <td>Baby</td>\n","      <td>$33552742.32</td>\n","      <td>21-1-1973</td>\n","      <td>United Kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Auburn National Bancorporation, Inc.</td>\n","      <td>ccaldeyroux5@dion.ne.jp</td>\n","      <td>Grocery</td>\n","      <td>$69798987.04</td>\n","      <td>19-9-1999</td>\n","      <td>U.K.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Interlink Electronics, Inc.</td>\n","      <td>orodenborch6@skyrock.com</td>\n","      <td>Garden</td>\n","      <td>$22521052.79</td>\n","      <td>8-6-2001</td>\n","      <td>SA</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>Tallgrass Energy Partners, LP</td>\n","      <td>NaN</td>\n","      <td>Grocery</td>\n","      <td>$54405380.40</td>\n","      <td>16-9-1992</td>\n","      <td>U.K/</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>Tronox Limited</td>\n","      <td>NaN</td>\n","      <td>Outdoors</td>\n","      <td>$99290004.13</td>\n","      <td>11-1-1992</td>\n","      <td>America</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Synopsys, Inc.</td>\n","      <td>lcancellieri9@tmall.com</td>\n","      <td>Electronics</td>\n","      <td>$44091294.62</td>\n","      <td>11-7-2006</td>\n","      <td>United Kingdom</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                            store_name               store_email  \\\n","0   1            Cullen/Frost Bankers, Inc.                       NaN   \n","1   2                   Nordson Corporation                       NaN   \n","2   3                 Stag Industrial, Inc.                       NaN   \n","3   4                   FIRST REPUBLIC BANK        ecanadine3@fc2.com   \n","4   5           Mercantile Bank Corporation                       NaN   \n","5   6  Auburn National Bancorporation, Inc.   ccaldeyroux5@dion.ne.jp   \n","6   7           Interlink Electronics, Inc.  orodenborch6@skyrock.com   \n","7   8         Tallgrass Energy Partners, LP                       NaN   \n","8   9                        Tronox Limited                       NaN   \n","9  10                        Synopsys, Inc.   lcancellieri9@tmall.com   \n","\n","    department        income date_measured          country  \n","0     Clothing  $54438554.24      4-2-2006   United States/  \n","1        Tools  $41744177.01      4-1-2006          Britain  \n","2       Beauty  $36152340.34     12-9-2003    United States  \n","3   Automotive   $8928350.04      8-5-2006         Britain/  \n","4         Baby  $33552742.32     21-1-1973   United Kingdom  \n","5      Grocery  $69798987.04     19-9-1999             U.K.  \n","6       Garden  $22521052.79      8-6-2001              SA   \n","7      Grocery  $54405380.40     16-9-1992             U.K/  \n","8     Outdoors  $99290004.13     11-1-1992          America  \n","9  Electronics  $44091294.62     11-7-2006   United Kingdom  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load up store_income_data.csv\n","store_income_df = pd.read_csv('store_income_data_task.csv')\n","store_income_df.head(10)\n"]},{"cell_type":"markdown","metadata":{"id":"ItqLwumA16Wr"},"source":["1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sLkzt4Hr16Wr"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 77 unique countries\n"]},{"data":{"text/plain":["array(['United States/', 'Britain', ' United States', 'Britain/',\n","       ' United Kingdom', 'U.K.', 'SA ', 'U.K/', 'America',\n","       'United Kingdom', nan, 'united states', ' S.A.', 'England ', 'UK',\n","       'S.A./', 'ENGLAND', 'BRITAIN', 'U.K', 'U.K ', 'America/', 'SA.',\n","       'S.A. ', 'u.k', 'uk', ' ', 'UK.', 'England/', 'england',\n","       ' Britain', 'united states of america', 'UK/', 'SA/', 'SA',\n","       'England.', 'UNITED KINGDOM', 'America.', 'S.A..', 's.a.', ' U.K',\n","       ' United States of America', 'Britain ', 'England', ' SA',\n","       'United States of America.', 'United States of America/',\n","       'United States.', 's. africasouth africa', ' England',\n","       'United Kingdom ', 'United States of America ', ' UK',\n","       'united kingdom', 'AMERICA', 'America ',\n","       'UNITED STATES OF AMERICA', ' S. AfricaSouth Africa', 'america',\n","       'S. AFRICASOUTH AFRICA', 'Britain.', '/', 'United Kingdom.',\n","       'United States', ' America', 'UNITED STATES', 'sa',\n","       'United States of America', 'UK ', 'United States ',\n","       'S. AfricaSouth Africa/', 'S.A.', 'United Kingdom/',\n","       'S. AfricaSouth Africa ', 'S. AfricaSouth Africa.',\n","       'S. AfricaSouth Africa', '.', 'britain'], dtype=object)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# display the unique values in country column in store_income_df\n","countries = store_income_df['country'].unique()\n","print(f\"There are {len(countries)} unique countries\")\n","countries\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 37 unique countries\n"]},{"data":{"text/plain":["array(['united states/', 'britain', 'united states', 'britain/',\n","       'united kingdom', 'u.k.', 'sa', 'u.k/', 'america', nan, 's.a.',\n","       'england', 'uk', 's.a./', 'u.k', 'america/', 'sa.', '', 'uk.',\n","       'england/', 'united states of america', 'uk/', 'sa/', 'england.',\n","       'america.', 's.a..', 'united states of america.',\n","       'united states of america/', 'united states.',\n","       's. africasouth africa', 'britain.', '/', 'united kingdom.',\n","       's. africasouth africa/', 'united kingdom/',\n","       's. africasouth africa.', '.'], dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# converting the country data to lower case and striping white spaces\n","store_income_df['country'] = store_income_df['country'].str.lower()\n","store_income_df['country'] = store_income_df['country'].str.strip()\n","\n","# display the unique countries\n","countries = store_income_df['country'].unique()\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"markdown","metadata":{"id":"P6dcDc4P16Ws"},"source":["2. Note that there should only be three separate countries. Eliminate all variations, so that 'South Africa', 'United Kingdom' and 'United States' are the only three countries."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qeV3CxMR16Ws"},"outputs":[],"source":["# Function to replace rows in the provided column of the provided DataFrame\n","# that match the provided string above the provided ratio with the provided string\n","def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n","    # get a list of unique strings\n","    strings = df[column].unique()\n","    \n","    # Get the top 10 closest matches to our input string\n","    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n","                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","    # Only get matches with a ratio > 90\n","    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n","\n","    # Get the rows of all the close matches in our dataframe\n","    rows_with_matches = df[column].isin(close_matches)\n","\n","    # Replace all rows with close matches with the input matches \n","    df.loc[rows_with_matches, column] = string_to_match\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 15 unique countries\n"]},{"data":{"text/plain":["array(['united states', 'britain', 'united kingdom', 'u.k', 'sa',\n","       'america', nan, 's.a.', 'england', 'uk', '',\n","       'united states of america', 's. africasouth africa', '/', '.'],\n","      dtype=object)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Calling function to replace for example uk/ as uk or uk. to uk \n","# Similar function done for all other countries and reduces unique values of \n","# country to 15 from 37\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"united kingdom\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"united states\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"south africa\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"united states of america\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"uk\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"u.k\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"sa\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"s.a.\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"s. africasouth africa\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"britain\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"england\")\n","replace_matches_in_column(df=store_income_df, column='country', string_to_match=\"america\")\n","\n","# get all the unique values in the 'country' column\n","countries = store_income_df['country'].unique()\n","\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 3 unique countries\n"]},{"data":{"text/plain":["array(['United Kingdom', 'South Africa', 'United States'], dtype=object)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Replacing all remaining redundant countries with either united kingdom,\n","# south africa or united states\n","store_income_df.replace('britain', 'united kingdom', inplace=True)\n","store_income_df.replace('u.k', 'united kingdom', inplace=True)\n","store_income_df.replace('sa', 'south africa', inplace=True)\n","store_income_df.replace('america', 'united states', inplace=True)\n","store_income_df.replace('s.a.', 'south africa', inplace=True)\n","store_income_df.replace('england', 'united kingdom', inplace=True)\n","store_income_df.replace('uk', 'united kingdom', inplace=True)\n","store_income_df.replace('united states of america', 'united states', inplace=True)\n","store_income_df.replace('s. africasouth africa', 'south africa', inplace=True)\n","\n","# Replacing all three countries in title case as given in question so changing\n","# from lowercase\n","store_income_df.replace('united kingdom','United Kingdom', inplace=True)\n","store_income_df.replace('united states','United States', inplace=True)\n","store_income_df.replace('south africa','South Africa', inplace=True)\n","\n","# Replacing / . and '' with nan as these rows have no country name prepping for drop\n","store_income_df.replace('/', np.nan, inplace=True)\n","store_income_df.replace('.', np.nan, inplace=True)\n","store_income_df.replace('', np.nan, inplace=True)\n","\n","# Dropping nan rows as these rows do not have country name\n","\n","store_income_df.dropna(inplace=True)\n","\n","# Get all the unique values in the 'country' column\n","countries = store_income_df['country'].unique()\n","\n","print(f\"There are {len(countries)} unique countries\")\n","countries"]},{"cell_type":"markdown","metadata":{"id":"UJZDMTwP16Ws"},"source":["3. Create a new column called `days_ago` in the DataFrame that is a copy of the 'date_measured' column but instead it is a number that shows how many days ago it was measured from the current date. Note that the current date can be obtained using `datetime.date.today()`."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gMJbN84P16Wt"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","      <th>date_parsed</th>\n","      <th>days_ago</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>United Kingdom</td>\n","      <td>2006-05-08</td>\n","      <td>6333</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Auburn National Bancorporation, Inc.</td>\n","      <td>ccaldeyroux5@dion.ne.jp</td>\n","      <td>Grocery</td>\n","      <td>$69798987.04</td>\n","      <td>19-9-1999</td>\n","      <td>United Kingdom</td>\n","      <td>1999-09-19</td>\n","      <td>8756</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Interlink Electronics, Inc.</td>\n","      <td>orodenborch6@skyrock.com</td>\n","      <td>Garden</td>\n","      <td>$22521052.79</td>\n","      <td>8-6-2001</td>\n","      <td>South Africa</td>\n","      <td>2001-06-08</td>\n","      <td>8128</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Synopsys, Inc.</td>\n","      <td>lcancellieri9@tmall.com</td>\n","      <td>Electronics</td>\n","      <td>$44091294.62</td>\n","      <td>11-7-2006</td>\n","      <td>United Kingdom</td>\n","      <td>2006-07-11</td>\n","      <td>6269</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>New Home Company Inc. (The)</td>\n","      <td>nhinchcliffef@whitehouse.gov</td>\n","      <td>Shoes</td>\n","      <td>$90808764.99</td>\n","      <td>21-4-1993</td>\n","      <td>United Kingdom</td>\n","      <td>1993-04-21</td>\n","      <td>11098</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id                            store_name                   store_email  \\\n","3    4                   FIRST REPUBLIC BANK            ecanadine3@fc2.com   \n","5    6  Auburn National Bancorporation, Inc.       ccaldeyroux5@dion.ne.jp   \n","6    7           Interlink Electronics, Inc.      orodenborch6@skyrock.com   \n","9   10                        Synopsys, Inc.       lcancellieri9@tmall.com   \n","15  16           New Home Company Inc. (The)  nhinchcliffef@whitehouse.gov   \n","\n","     department        income date_measured         country date_parsed  \\\n","3    Automotive   $8928350.04      8-5-2006  United Kingdom  2006-05-08   \n","5       Grocery  $69798987.04     19-9-1999  United Kingdom  1999-09-19   \n","6        Garden  $22521052.79      8-6-2001    South Africa  2001-06-08   \n","9   Electronics  $44091294.62     11-7-2006  United Kingdom  2006-07-11   \n","15        Shoes  $90808764.99     21-4-1993  United Kingdom  1993-04-21   \n","\n","    days_ago  \n","3       6333  \n","5       8756  \n","6       8128  \n","9       6269  \n","15     11098  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Get current date\n","current_date = pd.Timestamp.today()\n","\n","# Get date_measured in format dd-mm-yyyy and then subtract current date from this \n","# date and store it in new variable days_ago in store_income_df\n","store_income_df['date_parsed'] = pd.to_datetime(store_income_df['date_measured'], format='%d-%m-%Y')\n","store_income_df['date_parsed'].head()\n","store_income_df['days_ago']=(current_date - store_income_df['date_parsed']).dt.days\n","store_income_df.head()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('phd')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"63d17dc58a06b6a6d4136fb13c245dafcf53668da37b1c3052c24d689135f5bb"}}},"nbformat":4,"nbformat_minor":0}
